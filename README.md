# README

## Description
Given a list of urls, the application will navigate to each url, extract the public pages from the sitemap then proceed to crawl and extract all content to be saved into a text files.

## Installation

Follow these steps to install the application:

1. Clone the repository: `git clone <repository_url>`
2. Run `bash run.sh`
    2.1. If permissions issues run  `chmod +x run.sh`
## Usage

1. run `bash run.sh`
2. from the gui window enter the urls seperated by comma to begin scraping
